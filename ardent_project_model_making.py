# -*- coding: utf-8 -*-
"""ardent_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TukvwaSw59smNb1LxNbFqcTK-3XMdgzw
"""

!pip install transformers datasets pandas matplotlib seaborn

!pip install scikit-learn

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load CSV files
fake_df = pd.read_csv("Fake.csv")
true_df = pd.read_csv("True.csv")

# Add labels
fake_df["label"] = 0  # Fake news
true_df["label"] = 1  # Real news

# Combine datasets
df = pd.concat([fake_df, true_df], ignore_index=True)

# Shuffle rows
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Show structure
df.head()

df["content"] = df["title"] + " " + df["text"]

!pip install transformers datasets accelerate scikit-learn

from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

from torch.utils.data import Dataset
import torch

class FakeNewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        # Tokenize the text
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(),          # [512]
            "attention_mask": encoding["attention_mask"].squeeze(),# [512]
            "labels": torch.tensor(label, dtype=torch.long)
        }

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["content"], df["label"], test_size=0.2, random_state=42
)

# Reset index to avoid KeyError
train_texts = train_texts.reset_index(drop=True)
train_labels = train_labels.reset_index(drop=True)
val_texts = val_texts.reset_index(drop=True)
val_labels = val_labels.reset_index(drop=True)

train_dataset = FakeNewsDataset(train_texts, train_labels, tokenizer)
val_dataset = FakeNewsDataset(val_texts, val_labels, tokenizer)

from transformers import DistilBertForSequenceClassification

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2  # Binary classification: fake (0) vs real (1)
)

from sklearn.metrics import accuracy_score
import numpy as np

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    report_to=[]  # <---- disables W&B logging
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

import os

# Set a clean subfolder path under results
save_path = "./results/final_model"
os.makedirs(save_path, exist_ok=True)

# Save the model and tokenizer
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)

print(f"âœ… Final model saved to: {save_path}")

!zip -r final_model.zip ./results/final_model
from google.colab import files
files.download("final_model.zip")

# Get predictions on validation set
predictions = trainer.predict(val_dataset)

# Get predicted class labels (0 or 1)
y_pred = predictions.predictions.argmax(axis=1)

# Ground truth labels
y_true = predictions.label_ids

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Accuracy
acc = accuracy_score(y_true, y_pred)
print(f"âœ… Accuracy: {acc:.4f}")

# Full classification report
print("\nðŸ“Š Classification Report:")
print(classification_report(y_true, y_pred, target_names=["Fake", "Real"]))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Fake", "Real"], yticklabels=["Fake", "Real"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""pridiction!!!!"""

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast

model_path = "./results/final_model"  # or wherever you saved it

model = DistilBertForSequenceClassification.from_pretrained(model_path)
tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)

import torch

def predict_fake_news(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        prediction = torch.argmax(probs, dim=1).item()
        confidence = probs[0][prediction].item()

    label = "ðŸŸ¢ Real" if prediction == 1 else "ðŸ”´ Fake"
    print(f"Prediction: {label} ({confidence*100:.2f}% confidence)")
    return label

fake_article = """
NASA scientists have confirmed that the moon will collide with Earth in 2030 due to a sudden shift in gravitational pull.
This announcement was made following new satellite data released by a top-secret government mission called Project Eclipse.
The report, which was leaked to Reddit, also claims that world governments have been preparing underground bunkers to protect political elites.
"""
predict_fake_news(fake_article)

"""LIME!!!

"""

!pip install lime

from lime.lime_text import LimeTextExplainer
import numpy as np

class_names = ['Fake', 'Real']

def predict_proba_lime(texts):
    # Tokenize batch of texts
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()
    return probs

import numpy as np

def hf_predict_proba(texts):
    # Use the pipeline to get results
    results = classifier(texts)
    if isinstance(results, dict):  # Single input case
        results = [results]

    # Convert list of dicts to probability array
    probs = []
    for r in results:
        if r["label"] == "POSITIVE":
            probs.append([1 - r["score"], r["score"]])
        else:
            probs.append([r["score"], 1 - r["score"]])

    return np.array(probs)

from lime.lime_text import LimeTextExplainer

explainer = LimeTextExplainer(class_names=["Negative", "Positive"])

sample_text = "The vaccine contains a secret microchip used to track people globally."

exp = explainer.explain_instance(sample_text, hf_predict_proba, num_features=10)
exp.show_in_notebook(text=True)

"""# GRADIO"""

!pip install gradio --quiet

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast
import torch

model_path = "./results/final_model"  # path where you saved your model

model = DistilBertForSequenceClassification.from_pretrained(model_path)
tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)

# Set model to eval mode
model.eval()

def classify_news(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        confidence = probs[0].tolist()

    return {
        "Fake News": float(confidence[0]),
        "Real News": float(confidence[1])
    }

import gradio as gr

interface = gr.Interface(
    fn=classify_news,
    inputs=gr.Textbox(lines=10, placeholder="Paste news article here..."),
    outputs=gr.Label(num_top_classes=2),
    title="ðŸ“° AI-Powered Fake News Detector",
    description="Predicts whether a news article is real or fake, along with confidence scores."
)

interface.launch(share=True, debug=True)







!unzip final_model.zip -d final_model



from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast
import torch

model = DistilBertForSequenceClassification.from_pretrained("./final_model/results/final_model")
tokenizer = DistilBertTokenizerFast.from_pretrained("./final_model/results/final_model")

model.eval()

def classify_news(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        confidence = probs[0].tolist()

    return {
        "Fake News": float(confidence[0]),
        "Real News": float(confidence[1])
    }

import gradio as gr

interface = gr.Interface(
    fn=classify_news,
    inputs=gr.Textbox(lines=10, placeholder="Paste news article here..."),
    outputs=gr.Label(num_top_classes=2),
    title="ðŸ“° AI-Powered Fake News Detector",
    description="Predicts whether a news article is real or fake, along with confidence scores."
)
interface.launch(share=True, debug=True)